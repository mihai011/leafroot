---
version: '3.8'
x-airflow-common: &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
    build:
        context: .
        dockerfile: Dockerfile-airflow
        args:
            PYTHON_VERSION: ${PYTHON_VERSION}
            AIRFLOW_PROJ_DIR: ${AIRFLOW_PROJ_DIR}
    environment: &airflow-common-env
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
        AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
        AIRFLOW__CELERY__BROKER_URL: ${REDIS_PROTOCOL}://:@${REDIS_HOST}:6379/0

    volumes:
        - ./${AIRFLOW_PROJ_DIR}/dags:/opt/airflow/dags
        - ./${AIRFLOW_PROJ_DIR}/logs:/opt/airflow/logs
        - ./${AIRFLOW_PROJ_DIR}/config:/opt/airflow/config
        - ./${AIRFLOW_PROJ_DIR}/plugins:/opt/airflow/plugins
        - ./${AIRFLOW_PROJ_DIR}/scripts:/${AIRFLOW_PROJ_DIR}
    depends_on: &airflow-common-depends-on
        redis:
            condition: service_healthy
        db:
            condition: service_healthy

services:
    airflow-webserver:
        <<: *airflow-common
        command: [webserver]
        env_file:
            - ${ENV_FILE}
        ports:
            - 8080:8080
        healthcheck:
            test: [CMD, bash, scripts/healthcheck_webserver.sh]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - shared_network

    airflow-scheduler:
        <<: *airflow-common
        command: scheduler
        env_file:
            - ${ENV_FILE}
        healthcheck:
            test: [CMD, bash, scripts/healthcheck_scheduler.sh]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - shared_network

    airflow-worker:
        <<: *airflow-common
        command: worker
        env_file:
            - ${ENV_FILE}
        healthcheck:
            test: [CMD, bash, scripts/healthcheck_worker.sh]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        environment:
            <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
            DUMB_INIT_SETSID: '0'
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - shared_network

    airflow-triggerer:
        <<: *airflow-common
        command: triggerer
        env_file:
            - ${ENV_FILE}
        healthcheck:
            test: [CMD, bash, scripts/healthcheck_triggerer.sh]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - shared_network

    airflow-init:
        <<: *airflow-common
        command: init
        env_file:
            - ${ENV_FILE}
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_UPGRADE: 'true'
            _AIRFLOW_WWW_USER_CREATE: 'true'
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
            _PIP_ADDITIONAL_REQUIREMENTS: ''
        user: 0:0
        volumes:
            - ./${AIRFLOW_PROJ_DIR:-.}:/sources
        networks:
            - shared_network

    airflow-cli:
        <<: *airflow-common
        profiles:
            - debug
        env_file:
            - ${ENV_FILE}
        environment:
            <<: *airflow-common-env
            CONNECTION_CHECK_MAX_COUNT: '0'
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
        command:
            - bash
            - -c
            - sleep infinity
        networks:
            - shared_network

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
    flower:
        <<: *airflow-common
        command: flower
        env_file:
            - ${ENV_FILE}
        profiles:
            - flower
        ports:
            - 5555:5555
        healthcheck:
            test: [CMD, bash, scripts/healthcheck_flower.sh]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        networks:
            - shared_network
